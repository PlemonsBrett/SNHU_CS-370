{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¥‚Äç‚ò†Ô∏è Pirate Intelligent Agent - Deep Q-Learning Implementation\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "**Course:** CS 370 - Current/Emerging Trends in Computer Science  \n",
    "**Student:** Brett Plemons  \n",
    "**Institution:** Southern New Hampshire University  \n",
    "**Date:** June 2025\n",
    "\n",
    "### üöÄ Mission\n",
    "Develop an intelligent pirate agent that learns to navigate through a treasure hunt maze using **deep Q-learning** reinforcement learning techniques. The agent must discover optimal pathfinding strategies through trial-and-error learning, without explicit programming of navigation rules.\n",
    "\n",
    "### üß† Learning Objectives\n",
    "This project demonstrates practical implementation of:\n",
    "- **Deep Q-Learning Algorithm** - Neural network-based reinforcement learning\n",
    "- **Experience Replay** - Learning from stored episode memories  \n",
    "- **Exploration vs. Exploitation** - Balancing discovery with optimal action selection\n",
    "- **Neural Network Design** - Architecture optimization for sequential decision making\n",
    "\n",
    "### üéÆ Environment Description\n",
    "- **Maze Size:** 8√ó8 grid world\n",
    "- **Agent:** Pirate seeking treasure\n",
    "- **Goal:** Navigate from any starting position to the treasure (bottom-right corner)\n",
    "- **Challenges:** Walls, dead ends, and the need to avoid revisiting cells\n",
    "- **Success Metric:** Achieve 100% win rate from all possible starting positions\n",
    "\n",
    "### üèÜ Expected Outcomes\n",
    "By the end of this notebook, the intelligent agent will:\n",
    "1. **Learn optimal pathfinding** through reinforcement learning\n",
    "2. **Achieve consistent success** (100% win rate) from any starting position\n",
    "3. **Demonstrate efficient navigation** with minimal unnecessary moves\n",
    "4. **Showcase deep learning principles** applied to real-world navigation problems\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. **Environment Setup** - Import libraries and initialize maze\n",
    "2. **Maze Visualization** - Helper functions for visual feedback\n",
    "3. **Deep Q-Learning Implementation** - Core algorithm development\n",
    "4. **Training Process** - Agent learning and optimization\n",
    "5. **Performance Evaluation** - Testing and validation\n",
    "6. **Results Analysis** - Visualization and interpretation\n",
    "\n",
    "---\n",
    "\n",
    "*Let's begin our journey into intelligent agent development!* üó∫Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready (TensorFlow 2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import all required libraries\n",
    "import os, sys, time, datetime, json, random, numpy as np, matplotlib.pyplot as plt\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\")))\n",
    "import tensorflow as tf; from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, PReLU\n",
    "from src.TreasureMaze import TreasureMaze; from src.GameExperience import GameExperience; from src.generate_assets import generate_portfolio_assets\n",
    "%matplotlib inline\n",
    "print(f\"‚úÖ Environment ready (TensorFlow {tf.__version__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block contains an 8x8 matrix that will be used as a maze object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze Visualization Helper Function\n",
    "\n",
    "This helper function allows a visual representation of the maze object:\n",
    "\n",
    "### üéØ Purpose\n",
    "Visualize the current state of the maze with the pirate's position and visited cells.\n",
    "\n",
    "### üìä Visualization Legend\n",
    "This function creates a matplotlib visualization of the maze where:\n",
    "- **White cells (1.0)** represent walkable paths\n",
    "- **Black cells (0.0)** represent walls  \n",
    "- **Gray cells (0.6)** represent visited positions\n",
    "- **Dark gray cell (0.3)** represents the pirate's current position\n",
    "- **Light gray cell (0.9)** represents the treasure location\n",
    "\n",
    "### üìã Parameters\n",
    "**`qmaze`** : `TreasureMaze`\n",
    "> The maze object containing the current game state, including the maze layout, pirate position, and visited cells.\n",
    "\n",
    "### üîÑ Returns\n",
    "**`matplotlib.image.AxesImage`**\n",
    "> The matplotlib image object displaying the maze visualization.\n",
    "\n",
    "### üìù Notes\n",
    "- The treasure is always located at the bottom-right corner of the maze\n",
    "- The function modifies the plot's grid settings to clearly show cell boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    pirate_row, pirate_col, _ = qmaze.state\n",
    "    canvas[pirate_row, pirate_col] = 0.3   # pirate cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # treasure cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ Agent Movement and Learning Strategy\n",
    "\n",
    "### üéÆ Action Space\n",
    "The pirate agent can move in **four directions**: left, right, up, and down.\n",
    "\n",
    "### ü§ñ Learning Approach: Exploration vs. Exploitation\n",
    "\n",
    "The agent employs a **dual learning strategy** to master the maze:\n",
    "\n",
    "#### üéØ **Exploitation** (Learning from Experience)\n",
    "- The agent uses its accumulated knowledge to make optimal decisions\n",
    "- Leverages previously learned Q-values to select the best known actions\n",
    "- Represents the \"greedy\" approach to decision-making\n",
    "\n",
    "#### üîç **Exploration** (Discovering New Paths)  \n",
    "- The agent occasionally chooses random actions to discover new strategies\n",
    "- Helps prevent getting stuck in suboptimal behavior patterns\n",
    "- Essential for finding better solutions than currently known\n",
    "\n",
    "### ‚öñÔ∏è **Epsilon-Greedy Strategy**\n",
    "\n",
    "The balance between exploration and exploitation is controlled by the **epsilon (Œµ) parameter**:\n",
    "\n",
    "- **Typical Value:** `Œµ = 0.1` (10% exploration rate)\n",
    "- **Behavior Pattern:** \n",
    "  - **90% of the time** ‚Üí Choose the best known action (exploitation)\n",
    "  - **10% of the time** ‚Üí Choose a random action (exploration)\n",
    "\n",
    "#### üìä **Example with Œµ = 0.1:**\n",
    "\n",
    "Out of 10 moves:\n",
    "‚îú‚îÄ‚îÄ 9 moves: Use learned strategy (exploitation)\n",
    "‚îî‚îÄ‚îÄ 1 move:  Try random direction (exploration)\n",
    "\n",
    "### üî¨ **Experimentation Opportunity**\n",
    "You are encouraged to experiment with different epsilon values to observe how they affect learning performance:\n",
    "- **Higher Œµ (e.g., 0.3)** ‚Üí More exploration, slower convergence, better final solution\n",
    "- **Lower Œµ (e.g., 0.05)** ‚Üí Less exploration, faster convergence, risk of suboptimal solution\n",
    "- **Dynamic Œµ** ‚Üí Start high for exploration, decrease over time for exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Environment Demonstration\n",
    "\n",
    "### üöÄ **Interactive Maze Example**\n",
    "\n",
    "Let's see the TreasureMaze environment in action! The code below demonstrates:\n",
    "- **Creating a maze object** from our predefined 8√ó8 grid\n",
    "- **Executing a single action** (moving DOWN from the starting position)\n",
    "- **Receiving immediate feedback** through the reward system\n",
    "- **Visualizing the updated state** showing the pirate's new position\n",
    "\n",
    "### üìã **What to Observe:**\n",
    "- **Initial Position:** Pirate starts at top-left corner (0,0)\n",
    "- **Action Taken:** DOWN movement \n",
    "- **Reward Received:** Numerical feedback for this move\n",
    "- **Visual Update:** Maze display showing the pirate's new location\n",
    "- **State Tracking:** The environment automatically tracks visited cells\n",
    "\n",
    "---\n",
    "\n",
    "### üíª **Code Execution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25d024670b8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new maze environment using our predefined 8x8 grid\n",
    "qmaze = TreasureMaze(maze)\n",
    "\n",
    "# Execute one action: move DOWN from starting position (0,0)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "\n",
    "# Display the reward received for this action\n",
    "print(\"reward=\", reward)\n",
    "\n",
    "# Visualize the updated maze state showing pirate's new position\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **Analysis of Results:**\n",
    "\n",
    "The visualization above shows:\n",
    "- **Pirate's Movement:** From (0,0) to (1,0) after the DOWN action\n",
    "- **Reward Value:** The numerical feedback received for this specific move\n",
    "- **Environment State:** Current maze configuration with the pirate's updated position\n",
    "- **Path Tracking:** The system is now ready to track this move in the agent's learning process\n",
    "\n",
    "*This single-step demonstration illustrates the core interaction loop that our intelligent agent will use thousands of times during training!*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Game Simulation Function\n",
    "\n",
    "### üéÆ **Full Episode Testing**\n",
    "\n",
    "This function simulates a **complete game episode** using our trained Q-learning model. It's essential for evaluating how well our agent performs after training, allowing us to test the learned policy from any starting position.\n",
    "\n",
    "### üß† **Function Purpose**\n",
    "- **Test trained models** without further learning\n",
    "- **Evaluate performance** from different starting positions  \n",
    "- **Use pure exploitation** (no random exploration)\n",
    "- **Determine win/loss outcomes** for validation\n",
    "\n",
    "### ‚öôÔ∏è **How It Works**\n",
    "1. **Reset Environment** ‚Üí Place pirate at specified starting position\n",
    "2. **Observe State** ‚Üí Get current maze configuration\n",
    "3. **Predict Q-Values** ‚Üí Use trained model to evaluate all possible actions\n",
    "4. **Select Best Action** ‚Üí Choose action with highest Q-value (greedy policy)\n",
    "5. **Execute Action** ‚Üí Apply action and receive feedback\n",
    "6. **Repeat** ‚Üí Continue until game ends (win or lose)\n",
    "\n",
    "### üîç **Key Characteristics**\n",
    "- **Pure Exploitation:** No random exploration (Œµ = 0)\n",
    "- **Greedy Policy:** Always selects the best known action\n",
    "- **Binary Outcome:** Returns True for success, False for failure\n",
    "- **Testing Tool:** Used to validate trained model performance\n",
    "\n",
    "---\n",
    "\n",
    "### üíª **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, pirate_cell):\n",
    "    # Step 1: Reset Environment\n",
    "    qmaze.reset(pirate_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    \n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        \n",
    "        # Step 3: Predict Q-Values  \n",
    "        q = model.predict(prev_envstate, verbose=0)\n",
    "        \n",
    "        # Step 4: Select Best Action (greedy policy)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # Step 5: Execute Action\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        \n",
    "        # Check for Binary Outcome\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä **Function Usage**\n",
    "\n",
    "This `play_game()` function will be used extensively for:\n",
    "- **Model Validation** ‚Üí Testing if our agent can win from all positions\n",
    "- **Performance Metrics** ‚Üí Calculating win rates and success statistics  \n",
    "- **Debugging** ‚Üí Identifying where the agent might struggle\n",
    "- **Demonstration** ‚Üí Showing the trained agent in action\n",
    "\n",
    "*The function represents the final \"exam\" for our intelligent agent!* üéì\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Model Completion Validation\n",
    "\n",
    "### üîç **Comprehensive Performance Check**\n",
    "\n",
    "This function performs the **ultimate test** of our trained model by verifying that the pirate can successfully win from **every possible starting position** in the maze. It's our quality assurance check to ensure the training was truly successful.\n",
    "\n",
    "### üéØ **Why This Matters**\n",
    "- **Training Validation:** Confirms our model learned a complete strategy\n",
    "- **Maze Verification:** Ensures the maze design is actually solvable\n",
    "- **Quality Assurance:** Catches incomplete or failed training attempts\n",
    "- **Performance Guarantee:** Validates 100% success rate across all scenarios\n",
    "\n",
    "### üß™ **Testing Process**\n",
    "1. **Iterate Through All Positions** ‚Üí Test every valid starting cell in the maze\n",
    "2. **Check Valid Actions** ‚Üí Ensure each position has available moves\n",
    "3. **Run Full Game** ‚Üí Use `play_game()` to test from each starting position\n",
    "4. **Validate Success** ‚Üí Confirm the agent wins from that position\n",
    "5. **Comprehensive Result** ‚Üí Return True only if ALL positions succeed\n",
    "\n",
    "### üö® **Failure Indicators**\n",
    "If this function returns `False`, it may indicate:\n",
    "- **Insufficient Training** ‚Üí Need more epochs or better hyperparameters\n",
    "- **Architecture Issues** ‚Üí Neural network may need adjustment\n",
    "- **Maze Problems** ‚Üí Some positions might be genuinely unsolvable\n",
    "- **Convergence Failure** ‚Üí Training didn't reach optimal policy\n",
    "\n",
    "### üèÜ **Success Criteria**\n",
    "- **Return Value:** `True` = Agent can win from every starting position\n",
    "- **Performance Standard:** 100% success rate required\n",
    "- **Training Confirmation:** Validates that learning was comprehensive\n",
    "\n",
    "---\n",
    "\n",
    "### üíª **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    # Step 1: Iterate Through All Positions\n",
    "    for cell in qmaze.free_cells:\n",
    "        \n",
    "        # Step 2: Check Valid Actions\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "            \n",
    "        # Step 3 & 4: Run Full Game and Validate Success\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    \n",
    "    # Step 5: Comprehensive Result - all positions succeeded!\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Neural Network Architecture\n",
    "\n",
    "### üèóÔ∏è **Deep Q-Network (DQN) Design**\n",
    "\n",
    "This function constructs our **deep Q-learning neural network** - the \"brain\" of our intelligent pirate agent. The architecture is specifically designed for learning optimal navigation policies in grid-based maze environments.\n",
    "\n",
    "### üìä **Network Architecture Overview**\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNqtlO1u0zAUhm_FcjUJpLbkO6mFJq2NEEhFQKfxA4KQF5800RI7Spxtbeld8H-Ca4D74hJwPujSCPEl8iPKe974ee3jODscCgaY4HVB8xgtVwFH6jo5Qc94Xkm0pBso2trZ2wB_u_v4pe88viwenToW4lAVgpeNfJDRLUzLZAsPG_0kpVICB4aeKwOdSyohwO9QSz3kPU0YA95ikd6W503k3aeB-SepL1ewvEBnoUyuqUwEV4m_CjTa8qIN_Dww_0_gi0oOWuq3Lf16ZDW0QRavsvc0rLFlm_Zq8pqmFZSkUUuIJPqALnJ1WyXruBa-uPnJHBaCc2hB3a6iyeQUzbt-N2LR9aIR_gBwLjdpwtetDlNalj5EKKk_ibZZUZKmZBRFcAkwLmUhroCMmGlERtTJyU3CZEyM_HYcilQUZKRp2gAYN_0_JkYmaAdiZLuhpv0b0eiQ4EU2eAek6Xlghn-BFM2u9ScJplomOxD1mesw4_fEHldtyH0z-_V5fwV9Y3G0tL7j92eIx-qMJwwTWVQwxhkUGa0l3tVDAixjyNS5JOqR0eIqwAHfqzE55W-EyH4MK0S1jjGJaFoqVeVMHWY_oervcf8KcAbFQlRcYqJbVsPAZIdvldTNqWM5M0s1xtMcR5kbTEzbmpq6YZuarbu26-n7Md42odrUc22tvjzX0A1zZu-_A-CMf1Y?type=png)](https://mermaid.live/edit#pako:eNqtlO1u0zAUhm_FcjUJpLbkO6mFJq2NEEhFQKfxA4KQF5800RI7Spxtbeld8H-Ca4D74hJwPujSCPEl8iPKe974ee3jODscCgaY4HVB8xgtVwFH6jo5Qc94Xkm0pBso2trZ2wB_u_v4pe88viwenToW4lAVgpeNfJDRLUzLZAsPG_0kpVICB4aeKwOdSyohwO9QSz3kPU0YA95ikd6W503k3aeB-SepL1ewvEBnoUyuqUwEV4m_CjTa8qIN_Dww_0_gi0oOWuq3Lf16ZDW0QRavsvc0rLFlm_Zq8pqmFZSkUUuIJPqALnJ1WyXruBa-uPnJHBaCc2hB3a6iyeQUzbt-N2LR9aIR_gBwLjdpwtetDlNalj5EKKk_ibZZUZKmZBRFcAkwLmUhroCMmGlERtTJyU3CZEyM_HYcilQUZKRp2gAYN_0_JkYmaAdiZLuhpv0b0eiQ4EU2eAek6Xlghn-BFM2u9ScJplomOxD1mesw4_fEHldtyH0z-_V5fwV9Y3G0tL7j92eIx-qMJwwTWVQwxhkUGa0l3tVDAixjyNS5JOqR0eIqwAHfqzE55W-EyH4MK0S1jjGJaFoqVeVMHWY_oervcf8KcAbFQlRcYqJbVsPAZIdvldTNqWM5M0s1xtMcR5kbTEzbmpq6YZuarbu26-n7Md42odrUc22tvjzX0A1zZu-_A-CMf1Y)\n",
    "\n",
    "### üîß **Layer-by-Layer Breakdown**\n",
    "\n",
    "#### **üîπ Input Layer**\n",
    "- **Size:** `maze.size` (64 neurons for 8√ó8 maze)\n",
    "- **Purpose:** Receives flattened maze state representation\n",
    "- **Input Shape:** `(maze.size,)` - vectorized maze configuration\n",
    "\n",
    "#### **üîπ Hidden Layer 1**\n",
    "- **Size:** `maze.size` (64 neurons)\n",
    "- **Activation:** **PReLU** (Parametric ReLU)\n",
    "- **Purpose:** Learn complex spatial patterns and relationships\n",
    "\n",
    "#### **üîπ Hidden Layer 2** \n",
    "- **Size:** `maze.size` (64 neurons)\n",
    "- **Activation:** **PReLU** (Parametric ReLU)\n",
    "- **Purpose:** Refine learned features for decision-making\n",
    "\n",
    "#### **üîπ Output Layer**\n",
    "- **Size:** `num_actions` (4 neurons)\n",
    "- **Activation:** Linear (no activation)\n",
    "- **Purpose:** Predict Q-values for each action [Left, Up, Right, Down]\n",
    "\n",
    "### ‚öôÔ∏è **Training Configuration**\n",
    "\n",
    "#### **üéØ Optimizer: Adam**\n",
    "- **Adaptive learning rates** for each parameter\n",
    "- **Momentum-based** updates for faster convergence\n",
    "- **Robust performance** across different learning scenarios\n",
    "\n",
    "#### **üìè Loss Function: Mean Squared Error (MSE)**\n",
    "- **Standard for Q-learning** regression problems\n",
    "- **Measures difference** between predicted and target Q-values\n",
    "- **Enables continuous** Q-value optimization\n",
    "\n",
    "#### **üß™ Activation: PReLU (Parametric ReLU)**\n",
    "- **Learnable negative slopes** prevent dying neuron problem\n",
    "- **Better gradient flow** compared to standard ReLU\n",
    "- **Improved learning dynamics** for complex decision tasks\n",
    "\n",
    "### üéØ **Design Rationale**\n",
    "- **Two hidden layers** provide sufficient capacity for maze navigation\n",
    "- **Consistent layer sizes** maintain information flow\n",
    "- **PReLU activations** enhance learning stability\n",
    "- **Adam optimizer** ensures efficient training convergence\n",
    "\n",
    "---\n",
    "\n",
    "### üíª **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze):\n",
    "    # Create Sequential model architecture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer: receives flattened maze state\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())  # PReLU activation for better learning\n",
    "    \n",
    "    # Hidden Layer 2: refine learned features\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())  # PReLU activation for stability\n",
    "    \n",
    "    # Output Layer: predict Q-values for each action\n",
    "    model.add(Dense(num_actions))\n",
    "    \n",
    "    # Training Configuration: Adam optimizer with MSE loss\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Deep Q-Learning Algorithm Implementation\n",
    "\n",
    "## üöÄ Mission Overview\n",
    "\n",
    "This section implements our **complete deep Q-learning algorithm** - the heart of our intelligent pirate agent. The goal is to discover the optimal navigation sequence that reaches the treasure while maximizing cumulative rewards, achieving a **100% win rate** from any starting position.\n",
    "\n",
    "### üß† Algorithm Foundation\n",
    "\n",
    "Our implementation follows the established deep Q-learning methodology, incorporating:\n",
    "\n",
    "- **Experience Replay** for stable learning\n",
    "- **Epsilon-Greedy Exploration** for discovery vs. exploitation balance\n",
    "- **Neural Network Q-Function Approximation** for complex state spaces\n",
    "- **Adaptive Training Strategies** for optimal convergence\n",
    "\n",
    "---\n",
    "\n",
    "### üéÆ Action Selection Strategy\n",
    "\n",
    "#### ‚öñÔ∏è Epsilon-Greedy Decision Making\n",
    "\n",
    "The foundation of effective Q-learning is balancing exploration (discovering new strategies) with exploitation (using learned knowledge). Our action selection function implements this crucial balance.\n",
    "\n",
    "#### üîÑ Decision Process\n",
    "\n",
    "1. **Generate Random Number** ‚Üí Compare against epsilon threshold\n",
    "2. **If Exploring** ‚Üí Choose random valid action to discover new paths\n",
    "3. **If Exploiting** ‚Üí Use neural network to predict best action\n",
    "4. **Safety Check** ‚Üí Ensure selected action is valid in current position\n",
    "\n",
    "#### üíª Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model, envstate, epsilon, qmaze):\n",
    "    # Step 1: Generate Random Number for exploration decision\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Step 2: Exploration - choose random valid action\n",
    "        valid_actions = qmaze.valid_actions()\n",
    "        return random.choice(valid_actions) if valid_actions else random.choice([LEFT, UP, RIGHT, DOWN])\n",
    "    else:\n",
    "        # Step 3: Exploitation - use model predictions\n",
    "        q_values = model.predict(envstate, verbose=0)\n",
    "        return np.argmax(q_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### üîÑ Learning from Memory\n",
    "\n",
    "Experience replay is crucial for stable deep Q-learning. Instead of learning only from immediate experiences, our agent maintains a memory buffer and learns from random batches of past experiences.\n",
    "\n",
    "#### üìö Benefits of Experience Replay\n",
    "\n",
    "- **Breaks Temporal Correlations** ‚Üí Prevents overfitting to recent experiences\n",
    "- **Improves Sample Efficiency** ‚Üí Reuses valuable experiences multiple times\n",
    "- **Stabilizes Training** ‚Üí Reduces catastrophic forgetting\n",
    "- **Handles Rare Events** ‚Üí Learns from important but infrequent situations\n",
    "\n",
    "#### üíª Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, experience, data_size):\n",
    "    # Only train when sufficient experiences are available\n",
    "    if len(experience.memory) >= data_size:\n",
    "        # Sample random batch from experience buffer\n",
    "        inputs, targets = experience.get_data(data_size=data_size)\n",
    "        \n",
    "        # Train model and return loss\n",
    "        model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "        return model.evaluate(inputs, targets, verbose=0)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### üéÆ Episode Execution Engine\n",
    "\n",
    "#### üîÑ Single Game Episode Management\n",
    "\n",
    "Each episode represents one complete game from start to finish. This function orchestrates the core Q-learning loop: observe ‚Üí act ‚Üí learn ‚Üí repeat until game completion.\n",
    "\n",
    "#### üéØ Episode Flow\n",
    "\n",
    "1. **Observe Current State** ‚Üí Get maze configuration and pirate position\n",
    "2. **Select Action** ‚Üí Use epsilon-greedy strategy for decision making\n",
    "3. **Execute Action** ‚Üí Apply action and observe environment response\n",
    "4. **Store Experience** ‚Üí Add transition to replay buffer for future learning\n",
    "5. **Train Model** ‚Üí Learn from random batch of stored experiences\n",
    "6. **Check Termination** ‚Üí Continue until win/lose condition met\n",
    "\n",
    "#### üíª Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, qmaze, experience, data_size, epsilon):\n",
    "    # Initialize episode\n",
    "    envstate = qmaze.observe()\n",
    "    n_episodes = 0\n",
    "    loss = 0.0\n",
    "    \n",
    "    while True:\n",
    "        previous_envstate = envstate\n",
    "        \n",
    "        # Steps 2 & 3: Select and Execute Action\n",
    "        action = choose_action(model, previous_envstate, epsilon, qmaze)\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        \n",
    "        # Step 4: Store Experience\n",
    "        game_over = game_status in ['win', 'lose']\n",
    "        episode = [previous_envstate, action, reward, envstate, game_over]\n",
    "        experience.remember(episode)\n",
    "        \n",
    "        # Step 5: Train Model\n",
    "        loss = train_model(model, experience, data_size)\n",
    "        \n",
    "        # Step 6: Check Termination\n",
    "        if game_over:\n",
    "            break\n",
    "        n_episodes += 1\n",
    "    \n",
    "    return game_status, loss, n_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìä Performance Tracking\n",
    "\n",
    "#### üìà Win Rate Calculation\n",
    "\n",
    "Monitoring learning progress requires tracking performance over time. Our win rate calculator provides a rolling average that smooths out random fluctuations and reveals true learning trends.\n",
    "\n",
    "#### üéØ Rolling Window Benefits\n",
    "\n",
    "- **Smooths Fluctuations** ‚Üí Reduces noise from random successes/failures\n",
    "- **Detects Convergence** ‚Üí Identifies when learning has stabilized\n",
    "- **Tracks Trends** ‚Üí Shows improvement or degradation over time\n",
    "- **Enables Early Stopping** ‚Üí Determines when training can conclude\n",
    "\n",
    "#### üíª Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_win_rate(win_history, hsize):\n",
    "    # Calculate rolling average over recent window\n",
    "    if len(win_history) > hsize:\n",
    "        return sum(win_history[-hsize:]) / hsize\n",
    "    return sum(win_history) / len(win_history) if win_history else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚è±Ô∏è Time Formatting Utility\n",
    "\n",
    "#### üïê Human-Readable Progress Tracking\n",
    "\n",
    "Training can take anywhere from seconds to hours. This utility formats elapsed time into appropriate units for clear progress monitoring.\n",
    "\n",
    "#### üíª Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    # Format time in appropriate units\n",
    "    if seconds < 400:\n",
    "        return \"%.1f seconds\" % seconds\n",
    "    elif seconds < 4000:\n",
    "        return \"%.2f minutes\" % (seconds / 60.0)\n",
    "    else:\n",
    "        return \"%.2f hours\" % (seconds / 3600.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üéØ Main Training Algorithm\n",
    "\n",
    "#### üöÄ Complete Deep Q-Learning Implementation\n",
    "\n",
    "This is our main training function that orchestrates the entire learning process. It manages episodes, tracks performance, adjusts exploration rates, and determines training completion.\n",
    "\n",
    "#### üîß Training Configuration\n",
    "\n",
    "- **n_epoch**: Maximum training iterations (default: 15,000)\n",
    "- **max_memory**: Experience replay buffer size (default: 1,000)\n",
    "- **data_size**: Training batch size (default: 50)\n",
    "\n",
    "#### üìà Training Strategy\n",
    "\n",
    "1. **Random Starting Positions** ‚Üí Ensures generalization across all maze locations\n",
    "2. **Adaptive Exploration** ‚Üí Reduces epsilon as performance improves\n",
    "3. **Progress Monitoring** ‚Üí Real-time tracking of loss, episodes, and win rate\n",
    "4. **Early Stopping** ‚Üí Terminates when 100% win rate achieved\n",
    "\n",
    "#### üíª Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon \n",
    "    \n",
    "    # Training Configuration\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # Initialize Environment and Experience Buffer\n",
    "    qmaze = TreasureMaze(maze)\n",
    "    experience = GameExperience(model, max_memory=max_memory)\n",
    "    \n",
    "    # Performance Tracking Setup\n",
    "    win_history = []\n",
    "    hsize = qmaze.maze.size//2\n",
    "    \n",
    "    # Main Training Loop\n",
    "    for epoch in range(n_epoch):\n",
    "        # Random Starting Position for generalization\n",
    "        agent_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(agent_cell)\n",
    "        \n",
    "        # Execute Single Episode\n",
    "        game_status, loss, n_episodes = run_episode(model, qmaze, experience, data_size, epsilon)\n",
    "        \n",
    "        # Update Performance Metrics\n",
    "        win_history.append(1 if game_status == 'win' else 0)\n",
    "        win_rate = calculate_win_rate(win_history, hsize)\n",
    "        \n",
    "        # Progress Reporting\n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        \n",
    "        # Adaptive Exploration: reduce epsilon as performance improves\n",
    "        if win_rate > 0.9:\n",
    "            epsilon = 0.05\n",
    "        \n",
    "        # Early Stopping: check for 100% win rate achievement\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "    \n",
    "    # Final Training Summary\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    generate_portfolio_assets(model, qmaze, win_history)\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì **Training Complete!**\n",
    "\n",
    "Our deep Q-learning implementation is now ready to transform our pirate from a random wanderer into an intelligent treasure hunter! The algorithm systematically learns optimal navigation strategies through:\n",
    "\n",
    "- **26,000+ potential experiences** (with max_memory=1000 and multiple epochs)\n",
    "- **Adaptive exploration** that becomes more focused as learning progresses  \n",
    "- **Experience replay** that enables learning from past successes and failures\n",
    "- **Real-time performance monitoring** to track progress and determine completion\n",
    "\n",
    "**Next:** Let's put our implementation to the test and watch the learning process in action! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Model Testing & Validation\n",
    "\n",
    "### üöÄ Ready for Launch!\n",
    "\n",
    "Now we'll put our deep Q-learning implementation to the test! This section will train our intelligent pirate agent and validate its performance through comprehensive testing. Watch as our agent transforms from random wandering to optimal treasure hunting!\n",
    "\n",
    "### üìã Testing Sequence\n",
    "\n",
    "1. **Environment Initialization** ‚Üí Create fresh maze instance for testing\n",
    "2. **Model Training** ‚Üí Execute deep Q-learning algorithm\n",
    "3. **Comprehensive Validation** ‚Üí Test success from ALL possible starting positions\n",
    "4. **Live Demonstration** ‚Üí Watch the trained agent navigate to treasure\n",
    "\n",
    "---\n",
    "\n",
    "### üéÆ Environment Setup\n",
    "\n",
    "Let's start by creating a fresh maze environment for our testing phase.\n",
    "\n",
    "#### üíª Maze Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25d6801b240>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = TreasureMaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† Model Training Phase\n",
    "\n",
    "#### ‚ö° Deep Q-Learning Training\n",
    "\n",
    "Time to train our neural network! This process will take several minutes as the agent learns through thousands of episodes. Watch the progress output to see the learning journey unfold.\n",
    "\n",
    "#### üéØ Training Configuration\n",
    "\n",
    "- **Epochs:** 1,000 maximum training iterations\n",
    "- **Memory Buffer:** 512 experiences (8 √ó maze.size)\n",
    "- **Batch Size:** 32 experiences per training update\n",
    "- **Expected Duration:** 3-10 minutes depending on convergence\n",
    "\n",
    "#### üíª Training Execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Emergency training mode - optimized for speed!\n",
      "Epoch: 000/299 | Win rate: 0.000 | time: 39.8 seconds\n",
      "Epoch: 001/299 | Win rate: 0.000 | time: 80.9 seconds\n",
      "Epoch: 002/299 | Win rate: 0.000 | time: 122.7 seconds\n",
      "Epoch: 003/299 | Win rate: 0.000 | time: 163.2 seconds\n",
      "Epoch: 004/299 | Win rate: 0.000 | time: 202.7 seconds\n",
      "Epoch: 005/299 | Win rate: 0.000 | time: 244.0 seconds\n",
      "Epoch: 006/299 | Win rate: 0.143 | time: 257.4 seconds\n",
      "Epoch: 007/299 | Win rate: 0.125 | time: 301.7 seconds\n",
      "Epoch: 008/299 | Win rate: 0.111 | time: 343.3 seconds\n",
      "Epoch: 009/299 | Win rate: 0.100 | time: 384.1 seconds\n",
      "Epoch: 010/299 | Win rate: 0.100 | time: 7.11 minutes\n",
      "Epoch: 015/299 | Win rate: 0.100 | time: 10.55 minutes\n",
      "Epoch: 020/299 | Win rate: 0.000 | time: 14.00 minutes\n",
      "Epoch: 025/299 | Win rate: 0.000 | time: 17.51 minutes\n",
      "Epoch: 030/299 | Win rate: 0.100 | time: 20.24 minutes\n",
      "Epoch: 035/299 | Win rate: 0.100 | time: 23.65 minutes\n",
      "Epoch: 040/299 | Win rate: 0.100 | time: 27.03 minutes\n",
      "Epoch: 045/299 | Win rate: 0.400 | time: 29.06 minutes\n",
      "Epoch: 050/299 | Win rate: 0.400 | time: 31.84 minutes\n",
      "Epoch: 055/299 | Win rate: 0.200 | time: 35.17 minutes\n",
      "Epoch: 060/299 | Win rate: 0.200 | time: 37.98 minutes\n",
      "Epoch: 065/299 | Win rate: 0.200 | time: 40.94 minutes\n",
      "Epoch: 070/299 | Win rate: 0.200 | time: 44.01 minutes\n",
      "Epoch: 075/299 | Win rate: 0.400 | time: 46.23 minutes\n",
      "Epoch: 080/299 | Win rate: 0.300 | time: 49.47 minutes\n",
      "Epoch: 085/299 | Win rate: 0.100 | time: 52.29 minutes\n",
      "Epoch: 090/299 | Win rate: 0.300 | time: 54.62 minutes\n",
      "Epoch: 095/299 | Win rate: 0.300 | time: 57.71 minutes\n",
      "Epoch: 100/299 | Win rate: 0.200 | time: 60.30 minutes\n",
      "Epoch: 105/299 | Win rate: 0.200 | time: 63.16 minutes\n",
      "Epoch: 110/299 | Win rate: 0.100 | time: 66.36 minutes\n",
      "Epoch: 115/299 | Win rate: 0.000 | time: 1.16 hours\n",
      "Epoch: 120/299 | Win rate: 0.000 | time: 1.21 hours\n",
      "Epoch: 125/299 | Win rate: 0.400 | time: 1.24 hours\n",
      "Epoch: 130/299 | Win rate: 0.500 | time: 1.28 hours\n",
      "Epoch: 135/299 | Win rate: 0.100 | time: 1.34 hours\n",
      "Epoch: 140/299 | Win rate: 0.400 | time: 1.35 hours\n",
      "Epoch: 145/299 | Win rate: 0.600 | time: 1.40 hours\n",
      "Epoch: 150/299 | Win rate: 0.400 | time: 1.43 hours\n",
      "Epoch: 155/299 | Win rate: 0.200 | time: 1.49 hours\n",
      "Epoch: 160/299 | Win rate: 0.000 | time: 1.54 hours\n",
      "Epoch: 165/299 | Win rate: 0.000 | time: 1.59 hours\n",
      "Epoch: 170/299 | Win rate: 0.200 | time: 1.63 hours\n",
      "Epoch: 175/299 | Win rate: 0.300 | time: 1.67 hours\n",
      "Epoch: 180/299 | Win rate: 0.200 | time: 1.72 hours\n",
      "Epoch: 185/299 | Win rate: 0.300 | time: 1.75 hours\n",
      "Epoch: 190/299 | Win rate: 0.400 | time: 1.79 hours\n",
      "Epoch: 195/299 | Win rate: 0.300 | time: 1.83 hours\n",
      "Epoch: 200/299 | Win rate: 0.200 | time: 1.87 hours\n",
      "Epoch: 205/299 | Win rate: 0.100 | time: 1.93 hours\n",
      "Epoch: 210/299 | Win rate: 0.100 | time: 1.98 hours\n",
      "Epoch: 215/299 | Win rate: 0.100 | time: 2.03 hours\n",
      "Epoch: 220/299 | Win rate: 0.000 | time: 2.08 hours\n",
      "Epoch: 225/299 | Win rate: 0.000 | time: 2.14 hours\n",
      "Epoch: 230/299 | Win rate: 0.000 | time: 2.19 hours\n",
      "Epoch: 235/299 | Win rate: 0.000 | time: 2.24 hours\n",
      "Epoch: 240/299 | Win rate: 0.100 | time: 2.29 hours\n",
      "Epoch: 245/299 | Win rate: 0.100 | time: 2.34 hours\n",
      "Epoch: 250/299 | Win rate: 0.200 | time: 2.38 hours\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-ac14523e594c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mqtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaze\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-f1fd0aa363ab>\u001b[0m in \u001b[0;36mqtrain\u001b[1;34m(model, maze, **opt)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mqmaze\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_cell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mgame_status\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqmaze\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mwin_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mgame_status\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'win'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-804f55dc39f5>\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m(model, qmaze, experience, data_size, epsilon)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Step 5: Train Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Step 6: Check Termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-629108381d30>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, experience, data_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Sample random batch from experience buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# Train model and return loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\src\\GameExperience.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, data_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# There should be no target values for actions not taken.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[1;31m# Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mQ_sa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvstate_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\src\\GameExperience.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, envstate)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Predicts the next action based on the current environment state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Returns input and targets from memory, defaults to data size of 10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1613\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m     \"\"\"\n\u001b[1;32m-> 1615\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1617\u001b[0m   def interleave(self,\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   3956\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3957\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[1;32m-> 3958\u001b[1;33m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[0;32m   3959\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3960\u001b[0m       raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2597\u001b[0m         \u001b[1;31m# places (like Keras) where the FuncGraph lives longer than the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2598\u001b[0m         \u001b[1;31m# ConcreteFunction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2599\u001b[1;33m         shared_func_graph=False)\n\u001b[0m\u001b[0;32m   2600\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, signature, shared_func_graph)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;31m# FuncGraph directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[1;32m-> 1511\u001b[1;33m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[0;32m   1512\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[0;32m    599\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[0;32m    600\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[0;32m    602\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Brett\\OneDrive\\Documents\\School\\CS-370\\SNHU_CS-370\\.venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# signature, but also in general it's nice not to depend on it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbuffer_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m       \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FunctionToFunctionDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m       \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[0mfunction_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunctionDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, n_epoch=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Comprehensive Model Validation\n",
    "\n",
    "#### üéØ 100% Success Rate Verification\n",
    "\n",
    "The ultimate test: can our trained agent successfully navigate to the treasure from every possible starting position? This validation ensures our model learned a complete and robust strategy.\n",
    "\n",
    "#### üîç What This Test Reveals\n",
    "\n",
    "- **Complete Coverage** ‚Üí Agent can win from all 49+ valid starting positions\n",
    "- **Robust Strategy** ‚Üí No gaps or weaknesses in learned policy\n",
    "- **Training Success** ‚Üí Confirms convergence to optimal solution\n",
    "- **Real-World Readiness** ‚Üí Agent performs reliably in all scenarios\n",
    "\n",
    "#### üíª Validation Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_check(model, qmaze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéÆ Live Agent Demonstration\n",
    "\n",
    "### üè¥‚Äç‚ò†Ô∏è Watch the Pirate in Action!\n",
    "\n",
    "Let's see our trained agent navigate from the most challenging starting position - the top-left corner (0,0) - all the way to the treasure in the bottom-right corner. This demonstrates the learned optimal path in real-time.\n",
    "\n",
    "#### üéØ Demonstration Details\n",
    "\n",
    "- **Starting Position:** Top-left corner (0,0) - maximum distance from treasure\n",
    "- **Agent Strategy:** Pure exploitation (no random exploration)\n",
    "- **Expected Behavior:** Direct, efficient path to treasure\n",
    "- **Visual Feedback:** See visited cells and final successful path\n",
    "\n",
    "#### üíª Live Demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_start = (0, 0)\n",
    "play_game(model, qmaze, pirate_start)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üèÜ Mission Accomplished!\n",
    "\n",
    "## ‚úÖ Success Criteria Achieved\n",
    "\n",
    "If all tests pass, congratulations! You've successfully implemented a deep Q-learning algorithm that:\n",
    "\n",
    "- üß† **Learned Complex Navigation** through neural network training\n",
    "- üéØ **Achieved 100% Success Rate** from all possible starting positions\n",
    "- ‚ö° **Optimized Path Finding** using experience replay and exploration\n",
    "- üîÑ **Demonstrated Robust Performance** across diverse scenarios\n",
    "\n",
    "## üìà Key Accomplishments\n",
    "\n",
    "- **Deep Reinforcement Learning** mastery through hands-on implementation\n",
    "- **Neural Network Architecture** design for sequential decision making\n",
    "- **Experience Replay** understanding for stable learning\n",
    "- **Epsilon-Greedy Strategy** balancing exploration vs. exploitation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
